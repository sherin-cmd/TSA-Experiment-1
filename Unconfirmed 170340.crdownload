{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4856544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 45.1196\n",
      "Epoch 20/100, Loss: 38.9522\n",
      "Epoch 30/100, Loss: 34.4679\n",
      "Epoch 40/100, Loss: 30.9394\n",
      "Epoch 50/100, Loss: 28.0365\n",
      "Epoch 60/100, Loss: 25.5782\n",
      "Epoch 70/100, Loss: 23.4536\n",
      "Epoch 80/100, Loss: 21.5892\n",
      "Epoch 90/100, Loss: 19.9340\n",
      "Epoch 100/100, Loss: 18.4519\n",
      "\n",
      "Vector for 'dog':\n",
      " [ 0.75866395  0.38363544  0.54606463  0.28587111  0.14541439  0.01506681\n",
      "  0.81839196 -0.99366687 -0.10355316  0.63208182]\n",
      "\n",
      "Words similar to 'dog': [('quick', 0.2514146325285219), ('away', 0.2491524570031779), ('over', -0.09818765802572439), ('jumps', -0.10921522796889346), ('brown', -0.14500689980871748)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Parameters ---\n",
    "corpus = [\n",
    "    ['the', 'quick', 'brown', 'fox'],\n",
    "    ['jumps', 'over', 'the', 'lazy', 'dog'],\n",
    "    ['the', 'dog', 'barked'],\n",
    "    ['a', 'brown', 'dog', 'ran', 'away']\n",
    "]\n",
    "embedding_dim = 10\n",
    "window_size = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# --- Step 2: Vocabulary ---\n",
    "vocab = set(word for sentence in corpus for word in sentence)\n",
    "word2id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id2word = {idx: word for word, idx in word2id.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# --- Step 3: Generate training data ---\n",
    "def generate_training_data(corpus, window_size):\n",
    "    training_data = []\n",
    "    for sentence in corpus:\n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            context = []\n",
    "            for i in range(idx - window_size, idx + window_size + 1):\n",
    "                if i != idx and 0 <= i < len(sentence):\n",
    "                    context.append(sentence[i])\n",
    "            if context:\n",
    "                training_data.append((context, target_word))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(corpus, window_size)\n",
    "\n",
    "# --- Step 4: Initialize weights ---\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "# --- Step 5: Softmax ---\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "# --- Step 6: Forward propagation ---\n",
    "def forward(context_words):\n",
    "    context_ids = [word2id[w] for w in context_words]\n",
    "    x = np.mean(W1[context_ids], axis=0)\n",
    "    z = np.dot(x, W2)\n",
    "    y_pred = softmax(z)\n",
    "    return y_pred, x, context_ids\n",
    "\n",
    "# --- Step 7: Backward propagation ---\n",
    "def backward(x, y_pred, target_word):\n",
    "    target = np.zeros(vocab_size)\n",
    "    target[word2id[target_word]] = 1\n",
    "    error = y_pred - target\n",
    "    dW2 = np.outer(x, error)\n",
    "    dW1 = np.dot(W2, error)\n",
    "    return dW1, dW2\n",
    "\n",
    "# --- Step 8: Training ---\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        y_pred, x, context_ids = forward(context_words)\n",
    "        dW1, dW2 = backward(x, y_pred, target_word)\n",
    "        for context_id in context_ids:\n",
    "            W1[context_id] -= learning_rate * dW1 / len(context_ids)\n",
    "        W2 -= learning_rate * dW2\n",
    "        loss -= np.log(y_pred[word2id[target_word]] + 1e-7)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# --- Step 9: Get word vectors ---\n",
    "def get_word_vector(word):\n",
    "    return W1[word2id[word]] if word in word2id else None\n",
    "\n",
    "print(\"\\nVector for 'dog':\\n\", get_word_vector('dog'))\n",
    "\n",
    "# Optional: find most similar words by cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def most_similar(word, top_n=5):\n",
    "    if word not in word2id:\n",
    "        return []\n",
    "    word_vec = get_word_vector(word)\n",
    "    sims = [(other_word, cosine_similarity(word_vec, get_word_vector(other_word))) \n",
    "            for other_word in vocab if other_word != word]\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[:top_n]\n",
    "\n",
    "print(\"\\nWords similar to 'dog':\", most_similar('dog'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a63b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
